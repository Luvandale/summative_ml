# -*- coding: utf-8 -*-
"""Summative_Diana_Imali.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w1iz5a6bw_7xwkR5rkDmKN5lKZuaqHjE
"""

# Commented out IPython magic to ensure Python compatibility.
# importing the needed libraries
# %matplotlib inline

import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns
import cv2
import os
import glob
from lxml import etree
import xml.etree.ElementTree as ET
import lxml.etree
from xml.dom import minidom
from keras.models import Sequential
from keras.layers import Dense, Flatten
from keras.applications.vgg16 import VGG16
# !pip install easyocr
# !pip install imutils

# mounting my data on the drive
# from google.colab import drive
# drive.mount('/content/drive')

# reading my data from the folder that I had uploaded on the drive. The data consists of both images and xml files
for dirname, _, filenames in os.walk('/content/drive/MyDrive/ml_summative/archive (2)'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# resizing the images all the images and putting them in a variable X. We are resizing to shorten the training time and also to make sure that our images 
# are all the same size since the collected images may vary in size
resized_images  = 224

# this is the directory that contains all images
image_dr = "/content/drive/MyDrive/ml_summative/archive (2)/images" 
img_path = os.path.join(image_dr,'*g')
img_files = glob.glob(img_path)
# image sorting needs to be done to match the annotations of the bounding boxes
img_files.sort() 
img_list = []
for j in img_files:
    img = cv2.imread(j)
    img = cv2.resize(img, (resized_images,resized_images))
    img_list.append(np.array(img))

# trying to read one of the xml files with the annotations
import xml.etree.ElementTree as ET
mytree = ET.parse('/content/drive/MyDrive/ml_summative/archive (2)/annotations/Cars0.xml')
myroot = mytree.getroot()
print(myroot)

# because we resized the images we need to do the same for the annotations
def new_annotation(p):
    tree = etree.parse(p)
    for m in tree.xpath("size"):
        width = int(m.xpath("width")[0].text)
        height = int(m.xpath("height")[0].text)
    for m in tree.xpath("object/bndbox"):
        x_minimum = int(m.xpath("xmin")[0].text)/(width/resized_images)
        y_minimum = int(m.xpath("ymin")[0].text)/(height/resized_images)
        x_maximum = int(m.xpath("xmax")[0].text)/(width/resized_images)
        y_maximum = int(m.xpath("ymax")[0].text)/(height/resized_images)
    return [ int(x_minimum),int(y_minimum ),int( x_maximum ), int(y_maximum)]

path = '/content/drive/MyDrive/ml_summative/archive (2)/annotations'
text_files = ['/content/drive/MyDrive/ml_summative/archive (2)/annotations/' +p for p in sorted(os.listdir(path))]
ann_list=[]
for i in text_files:
  # print(i)
  
  ann_list.append(new_annotation(str(i)))

# the first element in the annotations folder with the different labels after resizing
ann_list[0]

# displaying the first 4 images
plt.figure(figsize=(10,20))
for i in range(0,4) :
    plt.subplot(10,5,i+1)
    plt.axis('off')
    plt.imshow(img_list[i])

#using open cv to detect the numberplate of the first image.Open cv is a library that is used in object detection
image_det = cv2.rectangle(img_list[2],(ann_list[2][0],ann_list[2][1]),(ann_list[2][2],ann_list[2][3]),(0, 0, 255))
plt.imshow(image_det)
plt.show()

#Turning our x and y into an array
X=np.array(img_list)
y=np.array(ann_list)

# doing data normalization to make sure each image has similar pixel distributions
X = X / 255
y = y / 255

# splitting the data into train test
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=1)

"""Instantiate a base model and load pre-trained weights into it.Freeze all layers in the base model by setting trainable = False"""

# Create the model
ANPR_model = Sequential()
ANPR_model.add(VGG16(weights="imagenet", include_top=False, input_shape=(resized_images, resized_images, 3)))
ANPR_model.add(Flatten())
ANPR_model.add(Dense(128, activation="relu"))
ANPR_model.add(Dense(128, activation="relu"))
ANPR_model.add(Dense(64, activation="relu"))
ANPR_model.add(Dense(4, activation="sigmoid"))

ANPR_model.layers[-6].trainable = False

ANPR_model.summary()
ANPR_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])

# HERE we are training only the top layer
train = ANPR_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=32, verbose=1)

ANPR_model.save('ANPR_model.h5', overwrite=True)

# Test
scores = ANPR_model.evaluate(X_test, y_test, verbose=0)
print("Score : %.2f%%" % (scores[1]*100))

test_loss, test_accuracy =ANPR_model.evaluate(X_test, y_test,steps=int(100))

print("Test results \n Loss:",test_loss,'\n Accuracy',test_accuracy)

"""unfreezing the model"""

# we are unfreezing the entire model and retraining at a lower learning rate
# Do a round of fine-tuning of the entire model
from tensorflow import keras
from tensorflow.keras import layers
# from keras.optimizers import SGD
ANPR_model.layers[-6].trainable = True
ANPR_model.summary()

ANPR_model.compile(
    optimizer=keras.optimizers.Adam(1e-5),  # Low learning rate
    loss=keras.losses.BinaryCrossentropy(from_logits=True),
    metrics=[keras.metrics.BinaryAccuracy()],
)

epochs = 10
ANPR_model.fit(X_train,y_train, epochs=epochs, validation_data=(X_val, y_val))

y_images = ANPR_model.predict(X_test)

# trying to check how the model is performing on the test data
plt.figure(figsize=(20,40))
for i in range(0,43) :
    plt.subplot(10,5,i+1)
    plt.axis('off')
    ny = y_images[i]*255
    image = cv2.rectangle(X_test[i],(int(ny[0]),int(ny[1])),(int(ny[2]),int(ny[3])),(0, 255, 0))
    plt.imshow(image)

"""Extraction using OCR"""

import cv2
from matplotlib import pyplot as plt
import numpy as np
import imutils
import easyocr

# we start off by reading in the image
e_image = cv2.imread('/content/drive/MyDrive/ml_summative/archive (2)/images/Cars1.png')
# we apply grascaling which basically stores values in a single array of black and white meaning this only 
# requires a single convolution in order to be executed unlike coloured images hence reducing computational resources
grayscaling = cv2.cvtColor(e_image, cv2.COLOR_BGR2GRAY)
# This is used in plotting the image
plt.imshow(cv2.cvtColor(grayscaling, cv2.COLOR_BGR2RGB))

n_reduction = cv2.bilateralFilter(grayscaling, 11, 17, 17) #This aids in Noise reduction
e_detection = cv2.Canny(n_reduction, 30, 200) #This is used for Edge detection
plt.imshow(cv2.cvtColor(e_detection, cv2.COLOR_BGR2RGB))

keypoints = cv2.findContours(e_detection.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
contours = imutils.grab_contours(keypoints)
contours = sorted(contours, key=cv2.contourArea, reverse=True)[:10]
location = None
for contour in contours:
    approx = cv2.approxPolyDP(contour, 10, True)
    if len(approx) == 4:
        location = approx
        break
mask = np.zeros(grayscaling.shape, np.uint8)
c_image = cv2.drawContours(mask, [location], 0,255, -1)
c_image = cv2.bitwise_and(e_image, e_image, mask=mask)
plt.imshow(cv2.cvtColor(c_image, cv2.COLOR_BGR2RGB))

(p,q) = np.where(mask==255)
(p1, q1) = (np.min(p), np.min(q))
(p2, q2) = (np.max(p), np.max(q))
cropped_image = grayscaling[p1:p2+1, q1:q2+1]
plt.imshow(cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB))

reader = easyocr.Reader(['en'])
result = reader.readtext(cropped_image)
result
text = result[0][-2]
font = cv2.FONT_HERSHEY_SIMPLEX
res = cv2.putText(e_image, text=text, org=(approx[0][0][0], approx[1][0][1]+60), fontFace=font, fontScale=1, color=(0,255,0), thickness=2, lineType=cv2.LINE_AA)
res = cv2.rectangle(e_image, tuple(approx[0][0]), tuple(approx[2][0]), (0,255,0),3)
plt.imshow(cv2.cvtColor(res, cv2.COLOR_BGR2RGB))